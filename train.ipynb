{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6043daf8-827b-42d1-9749-76b01ca70907",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#https://github.com/XuezheMax/apollo/blob/master/optim/apollo.py\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "\n",
    "class Apollo(Optimizer):\n",
    "    r\"\"\"Implements Atom algorithm.\n",
    "        Arguments:\n",
    "            params (iterable): iterable of parameters to optimize or dicts defining\n",
    "                parameter groups\n",
    "            lr (float): learning rate\n",
    "            beta (float, optional): coefficient used for computing running averages of gradient (default: 0.9)\n",
    "            eps (float, optional): term added to the denominator to improve numerical stability (default: 1e-4)\n",
    "            rebound (str, optional): recified bound for diagonal hessian:\n",
    "                ``'constant'`` | ``'belief'`` (default: None)\n",
    "            warmup (int, optional): number of warmup steps (default: 500)\n",
    "            init_lr (float, optional): initial learning rate for warmup (default: lr/1000)\n",
    "            weight_decay (float, optional): weight decay coefficient (default: 0)\n",
    "            weight_decay_type (str, optional): type of weight decay:\n",
    "                ``'L2'`` | ``'decoupled'`` | ``'stable'`` (default: 'L2')\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr, beta=0.9, eps=1e-4, rebound='constant', warmup=500, init_lr=None, weight_decay=0, weight_decay_type=None):\n",
    "        if not 0.0 < lr:\n",
    "            raise ValueError(\"Invalid learning rate value: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= beta < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(beta))\n",
    "        if rebound not in ['constant', 'belief']:\n",
    "            raise ValueError(\"Invalid recitifed bound: {}\".format(rebound))\n",
    "        if not 0.0 <= warmup:\n",
    "            raise ValueError(\"Invalid warmup updates: {}\".format(warmup))\n",
    "        if init_lr is None:\n",
    "            init_lr = lr / 1000\n",
    "        if not 0.0 <= init_lr <= lr:\n",
    "            raise ValueError(\"Invalid initial learning rate: {}\".format(init_lr))\n",
    "        if not 0.0 <= weight_decay:\n",
    "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
    "        if weight_decay_type is None:\n",
    "            weight_decay_type = 'L2' if rebound == 'constant' else 'decoupled'\n",
    "        if weight_decay_type not in ['L2', 'decoupled', 'stable']:\n",
    "            raise ValueError(\"Invalid weight decay type: {}\".format(weight_decay_type))\n",
    "\n",
    "        defaults = dict(lr=lr, beta=beta, eps=eps, rebound=rebound,\n",
    "                        warmup=warmup, init_lr=init_lr, base_lr=lr,\n",
    "                        weight_decay=weight_decay, weight_decay_type=weight_decay_type)\n",
    "        super(Apollo, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(Apollo, self).__setstate__(state)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg_grad'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['approx_hessian'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    # Previous update direction\n",
    "                    state['update'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "\n",
    "                # Calculate current lr\n",
    "                if state['step'] < group['warmup']:\n",
    "                    curr_lr = (group['base_lr'] - group['init_lr']) * state['step'] / group['warmup'] + group['init_lr']\n",
    "                else:\n",
    "                    curr_lr = group['lr']\n",
    "\n",
    "                # Perform optimization step\n",
    "                grad = p.grad\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Atom does not support sparse gradients.')\n",
    "\n",
    "                # Perform step weight decay\n",
    "                if group['weight_decay'] != 0 and group['weight_decay_type'] == 'L2':\n",
    "                    grad = grad.add(p, alpha=group['weight_decay'])\n",
    "\n",
    "                beta = group['beta']\n",
    "                eps = group['eps']\n",
    "                exp_avg_grad = state['exp_avg_grad']\n",
    "                B = state['approx_hessian']\n",
    "                d_p = state['update']\n",
    "\n",
    "                state['step'] += 1\n",
    "                bias_correction = 1 - beta ** state['step']\n",
    "                alpha = (1 - beta) / bias_correction\n",
    "\n",
    "                # calc the diff grad\n",
    "                delta_grad = grad - exp_avg_grad\n",
    "                if group['rebound'] == 'belief':\n",
    "                    rebound = delta_grad.norm(p=np.inf)\n",
    "                else:\n",
    "                    rebound = 0.01\n",
    "                    eps = eps / rebound\n",
    "\n",
    "                # Update the running average grad\n",
    "                exp_avg_grad.add_(delta_grad, alpha=alpha)\n",
    "\n",
    "                denom = d_p.norm(p=4).add(eps)\n",
    "                d_p.div_(denom)\n",
    "                v_sq = d_p.mul(d_p)\n",
    "                delta = delta_grad.div_(denom).mul_(d_p).sum().mul(-alpha) - B.mul(v_sq).sum()\n",
    "\n",
    "                # Update B\n",
    "                B.addcmul_(v_sq, delta)\n",
    "\n",
    "                # calc direction of parameter updates\n",
    "                if group['rebound'] == 'belief':\n",
    "                    denom = torch.max(B.abs(), rebound).add_(eps / alpha)\n",
    "                else:\n",
    "                    denom = B.abs().clamp_(min=rebound)\n",
    "\n",
    "                d_p.copy_(exp_avg_grad.div(denom))\n",
    "\n",
    "                # Perform step weight decay\n",
    "                if group['weight_decay'] != 0 and group['weight_decay_type'] != 'L2':\n",
    "                    if group['weight_decay_type'] == 'stable':\n",
    "                        weight_decay = group['weight_decay'] / denom.mean().item()\n",
    "                    else:\n",
    "                        weight_decay = group['weight_decay']\n",
    "                    d_p.add_(p, alpha=weight_decay)\n",
    "\n",
    "                p.add_(d_p, alpha=-curr_lr)\n",
    "\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0670bb0d-3301-45c2-9b1d-d199abcd28e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#https://github.com/issamemari/pytorch-multilabel-balanced-sampler/blob/master/sampler.py\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data.sampler import Sampler\n",
    "\n",
    "\n",
    "class MultilabelBalancedRandomSampler(Sampler):\n",
    "    \"\"\"\n",
    "    MultilabelBalancedRandomSampler: Given a multilabel dataset of length n_samples and\n",
    "    number of classes n_classes, samples from the data with equal probability per class\n",
    "    effectively oversampling minority classes and undersampling majority classes at the\n",
    "    same time. Note that using this sampler does not guarantee that the distribution of\n",
    "    classes in the output samples will be uniform, since the dataset is multilabel and\n",
    "    sampling is based on a single class. This does however guarantee that all classes\n",
    "    will have at least batch_size / n_classes samples as batch_size approaches infinity\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, labels, indices=None, class_choice=\"least_sampled\"):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "            labels: a multi-hot encoding numpy array of shape (n_samples, n_classes)\n",
    "            indices: an arbitrary-length 1-dimensional numpy array representing a list\n",
    "            of indices to sample only from\n",
    "            class_choice: a string indicating how class will be selected for every\n",
    "            sample:\n",
    "                \"least_sampled\": class with the least number of sampled labels so far\n",
    "                \"random\": class is chosen uniformly at random\n",
    "                \"cycle\": the sampler cycles through the classes sequentially\n",
    "        \"\"\"\n",
    "        self.labels = labels\n",
    "        self.indices = indices\n",
    "        if self.indices is None:\n",
    "            self.indices = range(len(labels))\n",
    "\n",
    "        self.num_classes = self.labels.shape[1]\n",
    "\n",
    "        # List of lists of example indices per class\n",
    "        self.class_indices = []\n",
    "        for class_ in range(self.num_classes):\n",
    "            lst = np.where(self.labels[:, class_] == 1)[0]\n",
    "            lst = lst[np.isin(lst, self.indices)]\n",
    "            self.class_indices.append(lst)\n",
    "\n",
    "        self.counts = [0] * self.num_classes\n",
    "\n",
    "        assert class_choice in [\"least_sampled\", \"random\", \"cycle\"]\n",
    "        self.class_choice = class_choice\n",
    "        self.current_class = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.count = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.count >= len(self.indices):\n",
    "            raise StopIteration\n",
    "        self.count += 1\n",
    "        return self.sample()\n",
    "\n",
    "    def sample(self):\n",
    "        class_ = self.get_class()\n",
    "        class_indices = self.class_indices[class_]\n",
    "        chosen_index = np.random.choice(class_indices)\n",
    "        if self.class_choice == \"least_sampled\":\n",
    "            for class_, indicator in enumerate(self.labels[chosen_index]):\n",
    "                if indicator == 1:\n",
    "                    self.counts[class_] += 1\n",
    "        return chosen_index\n",
    "\n",
    "    def get_class(self):\n",
    "        if self.class_choice == \"random\":\n",
    "            class_ = random.randint(0, self.labels.shape[1] - 1)\n",
    "        elif self.class_choice == \"cycle\":\n",
    "            class_ = self.current_class\n",
    "            self.current_class = (self.current_class + 1) % self.labels.shape[1]\n",
    "        elif self.class_choice == \"least_sampled\":\n",
    "            min_count = self.counts[0]\n",
    "            min_classes = [0]\n",
    "            for class_ in range(1, self.num_classes):\n",
    "                if self.counts[class_] < min_count:\n",
    "                    min_count = self.counts[class_]\n",
    "                    min_classes = [class_]\n",
    "                if self.counts[class_] == min_count:\n",
    "                    min_classes.append(class_)\n",
    "            try:\n",
    "                class_ = np.random.choice(min_classes)\n",
    "            except: pass\n",
    "        return class_\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "458bff3a-786b-4115-b2a5-30580b4557b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 1.1build1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from einops import rearrange\n",
    "from decord import VideoReader\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from segmentation_models_pytorch.losses import FocalLoss\n",
    "from transformers import AutoModel, AutoImageProcessor, AutoConfig\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorchvideo.transforms.transforms_factory import create_video_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7796eddf-604a-4166-8a65-d1109adf636c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"seed\":2023,\n",
    "    \"model_name\":\"facebook/timesformer-base-finetuned-k400\",\n",
    "    \"batch_size\":3,\n",
    "    \"learning_rate\":1e-5,\n",
    "    \"data_dir\":'../data/CarCollision',\n",
    "    \"checkpoint_dir\":'./checkpoint',\n",
    "    \"submission_dir\":'./submission',\n",
    "    # \"n_classes\":(2,3,4,3),\n",
    "    \"n_classes\":(2,3,4,3),\n",
    "    \"label_dict\":{\n",
    "        -1:[-1,-1,-1,-1],\n",
    "        0:[0,0,0,0],\n",
    "        1:[1,1,1,1],\n",
    "        2:[1,1,1,2],\n",
    "        3:[1,1,2,1],\n",
    "        4:[1,1,2,2],\n",
    "        5:[1,1,3,1],\n",
    "        6:[1,1,3,2],\n",
    "        7:[1,2,1,1],\n",
    "        8:[1,2,1,2],\n",
    "        9:[1,2,2,1],\n",
    "        10:[1,2,2,2],\n",
    "        11:[1,2,3,1],\n",
    "        12:[1,2,3,2]\n",
    "        # 0:[0,0,0,0],\n",
    "        # 1:[1,2,1,1],\n",
    "        # 2:[1,2,1,2],\n",
    "        # 3:[1,2,2,1],\n",
    "        # 4:[1,2,2,2],\n",
    "        # 5:[1,2,3,1],\n",
    "        # 6:[1,2,3,2],\n",
    "        # 7:[1,1,1,1],\n",
    "        # 8:[1,1,1,2],\n",
    "        # 9:[1,1,2,1],\n",
    "        # 10:[1,1,2,2],\n",
    "        # 11:[1,1,3,1],\n",
    "        # 12:[1,1,3,2]\n",
    "    },\n",
    "    \"label_reverse_dict\":{\n",
    "        # (0,0,0,0):0,\n",
    "        # (1,1,1,1):1,\n",
    "        # (1,1,1,2):2,\n",
    "        # (1,1,2,1):3,\n",
    "        # (1,1,2,2):4,\n",
    "        # (1,1,3,1):5,\n",
    "        # (1,1,3,2):6,\n",
    "        # (1,2,1,1):7,\n",
    "        # (1,2,1,2):8,\n",
    "        # (1,2,2,1):9,\n",
    "        # (1,2,2,2):10,\n",
    "        # (1,2,3,1):11,\n",
    "        # (1,2,3,2):12,\n",
    "        (0,0,0,0):0,\n",
    "        (1,2,1,1):1,\n",
    "        (1,2,1,2):2,\n",
    "        (1,2,2,1):3,\n",
    "        (1,2,2,2):4,\n",
    "        (1,2,3,1):5,\n",
    "        (1,2,3,2):6,\n",
    "        (1,1,1,1):7,\n",
    "        (1,1,1,2):8,\n",
    "        (1,1,2,1):9,\n",
    "        (1,1,2,2):10,\n",
    "        (1,1,3,1):11,\n",
    "        (1,1,3,2):12,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3aa4766-ab67-495c-af9b-2105955be99f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 2023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2023"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(config['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efc4ed4b-45cf-4be0-b294-561770a14b3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f\"{config['data_dir']}/train.csv\")\n",
    "test_df = pd.read_csv(f\"{config['data_dir']}/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "010565b3-ce05-46eb-a3cc-5ff6d2b892c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df['sample_id'] = train_df['sample_id'].apply(lambda x: int(x.split('_')[1]))\n",
    "test_df['sample_id'] = test_df['sample_id'].apply(lambda x: int(x.split('_')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80ccca81-afe9-4bcc-9a56-e73dcfa79865",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df['video_path'] = train_df['video_path'].apply(lambda x: config['data_dir'] + x[1:])\n",
    "test_df['video_path'] = test_df['video_path'].apply(lambda x: config['data_dir'] + x[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81166706-cd56-4958-828f-78db4aa1fc21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df['label']=-1\n",
    "test_df['label_split'] = test_df['label'].apply(config['label_dict'].get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "996bfb6a-12fe-4968-a5c7-67a42b02c9f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df['label_split'] = train_df['label'].apply(config['label_dict'].get)\n",
    "train_label_split = np.array(train_df['label_split'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bec6476-79ba-4412-bfd2-964cbeccb10f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_label_multi_hot = np.hstack([np.eye(n_class, dtype=np.int32)[train_label_split[:,idx]] for idx, n_class in enumerate(config['n_classes'])])\n",
    "train_df['label_multi_hot'] = train_label_multi_hot.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a3d8b75-3312-452a-94f4-c1313f9ce1c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>video_path</th>\n",
       "      <th>label</th>\n",
       "      <th>label_split</th>\n",
       "      <th>label_multi_hot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>../data/CarCollision/train/TRAIN_0000.mp4</td>\n",
       "      <td>7</td>\n",
       "      <td>[1, 1, 1, 1]</td>\n",
       "      <td>[0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>../data/CarCollision/train/TRAIN_0001.mp4</td>\n",
       "      <td>7</td>\n",
       "      <td>[1, 1, 1, 1]</td>\n",
       "      <td>[0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>../data/CarCollision/train/TRAIN_0002.mp4</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>../data/CarCollision/train/TRAIN_0003.mp4</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>../data/CarCollision/train/TRAIN_0004.mp4</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 2, 1, 1]</td>\n",
       "      <td>[0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2693</th>\n",
       "      <td>2693</td>\n",
       "      <td>../data/CarCollision/train/TRAIN_2693.mp4</td>\n",
       "      <td>3</td>\n",
       "      <td>[1, 2, 2, 1]</td>\n",
       "      <td>[0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2694</th>\n",
       "      <td>2694</td>\n",
       "      <td>../data/CarCollision/train/TRAIN_2694.mp4</td>\n",
       "      <td>5</td>\n",
       "      <td>[1, 2, 3, 1]</td>\n",
       "      <td>[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>2695</td>\n",
       "      <td>../data/CarCollision/train/TRAIN_2695.mp4</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>2696</td>\n",
       "      <td>../data/CarCollision/train/TRAIN_2696.mp4</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>2697</td>\n",
       "      <td>../data/CarCollision/train/TRAIN_2697.mp4</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2698 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sample_id                                 video_path  label  \\\n",
       "0             0  ../data/CarCollision/train/TRAIN_0000.mp4      7   \n",
       "1             1  ../data/CarCollision/train/TRAIN_0001.mp4      7   \n",
       "2             2  ../data/CarCollision/train/TRAIN_0002.mp4      0   \n",
       "3             3  ../data/CarCollision/train/TRAIN_0003.mp4      0   \n",
       "4             4  ../data/CarCollision/train/TRAIN_0004.mp4      1   \n",
       "...         ...                                        ...    ...   \n",
       "2693       2693  ../data/CarCollision/train/TRAIN_2693.mp4      3   \n",
       "2694       2694  ../data/CarCollision/train/TRAIN_2694.mp4      5   \n",
       "2695       2695  ../data/CarCollision/train/TRAIN_2695.mp4      0   \n",
       "2696       2696  ../data/CarCollision/train/TRAIN_2696.mp4      0   \n",
       "2697       2697  ../data/CarCollision/train/TRAIN_2697.mp4      0   \n",
       "\n",
       "       label_split                       label_multi_hot  \n",
       "0     [1, 1, 1, 1]  [0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0]  \n",
       "1     [1, 1, 1, 1]  [0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0]  \n",
       "2     [0, 0, 0, 0]  [1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0]  \n",
       "3     [0, 0, 0, 0]  [1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0]  \n",
       "4     [1, 2, 1, 1]  [0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0]  \n",
       "...            ...                                   ...  \n",
       "2693  [1, 2, 2, 1]  [0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0]  \n",
       "2694  [1, 2, 3, 1]  [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0]  \n",
       "2695  [0, 0, 0, 0]  [1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0]  \n",
       "2696  [0, 0, 0, 0]  [1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0]  \n",
       "2697  [0, 0, 0, 0]  [1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0]  \n",
       "\n",
       "[2698 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ec5e7d4-9538-46a1-a9ee-2b497e9b13c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df_for_dataset, _ , val_df_for_dataset, _  = iterative_train_test_split(X=train_df.values, y=train_label_multi_hot, test_size=0.2)\n",
    "test_df_for_dataset = test_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d0e0e8d-b4e5-4512-a9ab-428c38d3fcd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_multi_hot_for_sampler = np.array(train_df_for_dataset[:,4].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d904697-b9e9-4c5c-b968-9c6363ef75c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, df_for_dataset, transform=None):\n",
    "        self.sample_id = df_for_dataset[:,0]\n",
    "        self.video_path = df_for_dataset[:,1]\n",
    "        self.label = df_for_dataset[:,2]\n",
    "        self.label_split = np.array(df_for_dataset[:,3].tolist())\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_id)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_id = self.sample_id[idx]\n",
    "        video_path = self.video_path[idx]\n",
    "        vr = VideoReader(video_path)\n",
    "        video = torch.from_numpy(vr.get_batch(range(50)).asnumpy())\n",
    "        video = rearrange(video, 't h w c -> c t h w')\n",
    "        label = self.label[idx]\n",
    "        label_split = self.label_split[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            video = self.transform(video)\n",
    "        video = rearrange(video, 'c t h w -> t c h w')\n",
    "\n",
    "        sample = {\n",
    "            'sample_id':sample_id,\n",
    "            'video':video,\n",
    "            'label':label,\n",
    "            'label_split':label_split\n",
    "        }\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44412c6c-571b-424d-abcf-31e85c7a6451",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_config = AutoConfig.from_pretrained(config['model_name'])\n",
    "image_processor_config = AutoImageProcessor.from_pretrained(config['model_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "980cf201-a2db-40a9-9a7b-06c17beb46c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_transform = create_video_transform(\n",
    "    mode='train',\n",
    "    num_samples=model_config.num_frames,\n",
    "    video_mean = tuple(image_processor_config.image_mean),\n",
    "    video_std = tuple(image_processor_config.image_std),\n",
    "    crop_size = tuple(image_processor_config.crop_size.values())\n",
    ")\n",
    "\n",
    "val_transform = create_video_transform(\n",
    "    mode='val',\n",
    "    num_samples=model_config.num_frames,\n",
    "    video_mean = tuple(image_processor_config.image_mean),\n",
    "    video_std = tuple(image_processor_config.image_std),\n",
    "    crop_size = tuple(image_processor_config.crop_size.values())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73051c54-d98d-4116-a2d1-58b7d0b4ea41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = VideoDataset(train_df_for_dataset, transform=train_transform)\n",
    "val_dataset = VideoDataset(val_df_for_dataset, transform=val_transform)\n",
    "test_dataset = VideoDataset(test_df_for_dataset, transform=val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70ba5aeb-8675-4560-b418-6eaca1a63baf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_sampler = MultilabelBalancedRandomSampler(train_multi_hot_for_sampler)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size= config['batch_size'], sampler=train_sampler)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size = config['batch_size']*2)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = config['batch_size']*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bab5e5f9-598b-4e53-aad7-c464aa533bd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PLVideoModel(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.learning_rate = config['learning_rate']\n",
    "        self.model = AutoModel.from_pretrained(config['model_name'])\n",
    "        self.classifiers = nn.ModuleList([\n",
    "            nn.LazyLinear(n_class) for n_class in config['n_classes']\n",
    "        ])\n",
    "        self.loss = FocalLoss('multiclass')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x).last_hidden_state.mean(dim=1)\n",
    "        x_out = [classifier(x) for classifier in self.classifiers]\n",
    "        return x_out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        video, label, label_split = batch['video'], batch['label'], batch['label_split']\n",
    "        y_hats = self.forward(batch[\"video\"])\n",
    "        loss = sum([self.loss(y_hats[i], batch[\"label_split\"][:,i]) for i in range(len(self.config['n_classes']))])\n",
    "        loss = loss/len(self.config['n_classes'])\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        video, label, label_split = batch['video'], batch['label'], batch['label_split']\n",
    "        y_hats = self.forward(batch[\"video\"])\n",
    "        step_output = [*y_hats, label]\n",
    "        return step_output\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        video, _, _ = batch['video'], batch['label'], batch['label_split']\n",
    "        y_hats = self.forward(batch[\"video\"])\n",
    "        step_output = y_hats\n",
    "        return step_output\n",
    "\n",
    "    def validation_epoch_end(self, step_outputs):\n",
    "        pred1, pred2, pred3, pred4, label = [], [], [], [], []\n",
    "        for step_output in step_outputs:\n",
    "            pred1.append(step_output[0])\n",
    "            pred2.append(step_output[1])\n",
    "            pred3.append(step_output[2])\n",
    "            pred4.append(step_output[3])\n",
    "            label.append(step_output[4])\n",
    "            \n",
    "        pred1 = torch.cat(pred1).argmax(1)\n",
    "        pred2 = torch.cat(pred2).argmax(1)\n",
    "        pred3 = torch.cat(pred3).argmax(1)\n",
    "        pred4 = torch.cat(pred4).argmax(1)\n",
    "        label = torch.cat(label).tolist()\n",
    "\n",
    "        pred = torch.stack([pred1,pred2,pred3,pred4],dim=1).cpu().detach().numpy().tolist()\n",
    "        pred = list(map(lambda x: self.config['label_reverse_dict'].get(tuple(x),0),pred))\n",
    "        \n",
    "        score = f1_score(label,pred, average='macro')\n",
    "        self.log(\"val_score\", score)\n",
    "        return score\n",
    "    \n",
    "    def post_preproc(self, step_outputs):\n",
    "        pred1, pred2, pred3, pred4 = [], [], [], []\n",
    "        for step_output in step_outputs:\n",
    "            pred1.append(step_output[0])\n",
    "            pred2.append(step_output[1])\n",
    "            pred3.append(step_output[2])\n",
    "            pred4.append(step_output[3])\n",
    "            \n",
    "        pred1 = torch.cat(pred1).argmax(1)\n",
    "        pred2 = torch.cat(pred2).argmax(1)\n",
    "        pred3 = torch.cat(pred3).argmax(1)\n",
    "        pred4 = torch.cat(pred4).argmax(1)\n",
    "\n",
    "        pred = torch.stack([pred1,pred2,pred3,pred4],dim=1).cpu().detach().numpy().tolist()\n",
    "        pred = list(map(lambda x: self.config['label_reverse_dict'].get(tuple(x),0),pred))\n",
    "\n",
    "        return pred\n",
    "            \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Apollo(self.parameters(), lr=self.learning_rate)\n",
    "        return [optimizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de111177-f1dd-4015-90f4-514b9ee8cfe2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/timesformer-base-finetuned-k400 were not used when initializing TimesformerModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing TimesformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TimesformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "Using 16bit None Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4070 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /mnt/c/Users/DODAM/Desktop/Project/CarCollision/checkpoint exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/model_summary/model_summary.py:411: UserWarning: A layer with UninitializedParameter was found. Thus, the total number of parameters detected may be inaccurate.\n",
      "  warning_cache.warn(\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | model       | TimesformerModel | 121 M \n",
      "1 | classifiers | ModuleList       | 0     \n",
      "2 | loss        | FocalLoss        | 0     \n",
      "-------------------------------------------------\n",
      "121 M     Trainable params\n",
      "0         Non-trainable params\n",
      "121 M     Total params\n",
      "242.518   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81815d2a5fae4c7c95fe94e4a31c3e87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=15` reached.\n"
     ]
    }
   ],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_score',\n",
    "    dirpath=config['checkpoint_dir'],\n",
    "    filename=f'{config[\"model_name\"]}'+'-{epoch:02d}-{train_loss:.4f}-{val_score:.4f}',\n",
    "    mode='max'\n",
    ")\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"train_loss\",\n",
    "    patience=3,\n",
    "    verbose=False,\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "pl_video_model = PLVideoModel(config)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    accelerator='auto', \n",
    "    precision=16,\n",
    "    callbacks=[early_stop_callback, checkpoint_callback]            \n",
    ")\n",
    "trainer.fit(pl_video_model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cfc34192-1678-474a-9b5f-1822995968ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/timesformer-base-finetuned-k400 were not used when initializing TimesformerModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing TimesformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TimesformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4070 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5b78dd9d6d4dfaaffaf78e53a8f309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pl_video_model_pretrained = PLVideoModel.load_from_checkpoint(\n",
    "    \"checkpoint/facebook/timesformer-base-finetuned-k400-epoch=13-train_loss=0.1063-val_score=0.4970.ckpt\",\n",
    "    config=config\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(accelerator='auto')\n",
    "pred = trainer.predict(pl_video_model_pretrained, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79694e91-7603-4e95-96b7-21cfb2eea336",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_post_proc = pl_video_model_pretrained.post_preproc(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6de3d0c7-e470-439f-8522-336754e26875",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "submit = pd.read_csv(f\"{config['data_dir']}/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f3c6610-75ea-4b69-8647-c8d6625d761a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "submit['label'] = pred_post_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5fadb94c-0844-40ee-9864-942e42517b61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "submit.to_csv(f\"{config['submission_dir']}/submission01.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
